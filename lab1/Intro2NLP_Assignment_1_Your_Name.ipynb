{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a325897",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing: Assignment 1\n",
    "\n",
    "In this assignment we'll practice tokenization, lemmatization and stemming\n",
    "\n",
    "- Please comment your code\n",
    "- Submissions are due Sunday at 23:59 **only** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 23.04.2023, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately \"Assignment_1_\\<Your_Name\\>.ipynb\".\n",
    "- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8bf33",
   "metadata": {},
   "source": [
    "### Task 1.1 (3 points)\n",
    "\n",
    "Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n",
    "1. num_words: The number of words in string\n",
    "2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n",
    "\n",
    "**Hint:** The string can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d49c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14f3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_tokens(any_string):\n",
    "    #here comes your code\n",
    "    num_words = len(re.findall(r\"\\w+(?:[-']\\w+)*\", any_string)) #if regex is forbidden, I would have chosen text.split(\" \")\n",
    "    # this regex separates on spaces and special characters, but words like \"don't\" or on-site will be counted as one word.\n",
    "    #num_words = len(any_string.split(\" \")) \n",
    "    num_tokens = len([x for x in any_string if x != ' '])\n",
    "    print([x for x in any_string if x != ' ']) # character-based tokenization\n",
    "    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e324a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "Hey, How are you doing? Oh lately I've been busy! And you? I have been working on-site! : num_words: 17 and num_tokens: 71 respectively\n"
     ]
    }
   ],
   "source": [
    "extract_words_tokens(\"Hey, How are you doing? Oh lately I've been busy! And you? I have been working on-site!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b05add",
   "metadata": {},
   "source": [
    "### Task 1.2 (4 points)\n",
    "\n",
    "Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n",
    "\n",
    "**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12f48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(any_string, file_name):\n",
    "    #here comes your code\n",
    "    lemma_dict = {}\n",
    "    with open(file_name, \"r\", encoding='utf-8-sig') as infile: \n",
    "        for line in infile: \n",
    "            value, key = line.strip().split(\"\\t\")\n",
    "            lemma_dict[key] = value\n",
    "    \n",
    "    lower_string = any_string.lower() #since every word in lemmantization file is in lowercase, i use lower() method\n",
    "    tokenized_string = lower_string.split(\" \")\n",
    "    dictionary_of_lemmatized_words = {}\n",
    "    for word in tokenized_string:\n",
    "        dictionary_of_lemmatized_words[word] = lemma_dict.get(word, word) # in case word isn't in lemma_dict\n",
    "\n",
    "    \n",
    "        \n",
    "    #print(dictionary_of_lemmatized_words) \n",
    "    return(print(dictionary_of_lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee899731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'there': 'there', 'is': 'be', 'an': 'a', 'alerting': 'alert', 'rise': 'rise', 'of': 'of', 'fires': 'fire', 'in': 'in', 'californian': 'californian', 'forests': 'forest'}\n"
     ]
    }
   ],
   "source": [
    "lemmatize(\"There is an alerting rise of fires in Californian forests\", \"lemmatization-en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266bdc4",
   "metadata": {},
   "source": [
    "### Task 1.3 (3 points)\n",
    "\n",
    "Write a function `stemmer(string)` that takes a string as input and returns a string processed with its stem.\n",
    "\n",
    "Create rules for the following cases as an example:\n",
    "\n",
    "- study - studi\n",
    "- studies - studi\n",
    "- studying - studi\n",
    "- studied - studi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5c587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(any_string):\n",
    "    #here comes your code\n",
    "    if any_string.endswith('y'):\n",
    "        stemmed_string = re.sub('y$', 'i', any_string)\n",
    "    elif any_string.endswith('es'):\n",
    "        stemmed_string = re.sub('es$', '', any_string)\n",
    "    elif any_string.endswith('ying'):\n",
    "        stemmed_string = re.sub('ying$', 'i', any_string)\n",
    "    elif any_string.endswith('ed'):\n",
    "        stemmed_string = re.sub('ed$', '', any_string)\n",
    "    return(print(stemmed_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6cc405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "studi\n",
      "studi\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "stemmer('studies')\n",
    "stemmer('studying')\n",
    "stemmer('study')\n",
    "stemmer('studied')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
